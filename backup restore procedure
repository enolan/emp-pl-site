So if we weren't using GKE, restoring a disk from a gcloud snapshot would just
be deleting the disk and creating a new one with the same name based on the
snapshot. But since we're using kube and dynamic provisioning, messing with the
PDs manually is probably undefined behavior. Hence the below janky method.

1. Figure out which gcloud PD corresponds to the kube PV created by dynamic
   provisioning triggered by the PVC you want to restore to. Run `kubectl get pv`.
   The right one will have "$NAMESPACE/postgres-pv-claim" in the CLAIM column.
2. In the gcloud web console, find the Compute Engine > Snapshots page. Find the
   latest (or last good) snapshot for the disk whose name you found in step 1.
3. Delete the Postgres deployment to free up the PD. `kubectl delete deployment postgres`.
   The gcloud web console should now show the target disk isn't bound to an
   instance.
4. Create a VM instance manually with the gcloud web console. It should be in
   the `us-west1-b` region. Attach the target disk, and also create and attach a
   disk based on the snapshot. Set that disk to delete when the instance is
   deleted.
5. SSH into the instance by copying the gcloud command from the web console. The
   web based SSH is broken.
6. Find the disk generated from the snapshot and the target disk in
   `/dev/disk/by-id`. They're listed there by their GCE names.
7. run `dd if=/dev/disk/by-id/$SNAPSHOT_DISK of=/dev/disk/by-id/$TARGET_DISK bs=1M &`
8. Note the PID and run `kill -USR1 $PID` periodically to watch progress.
